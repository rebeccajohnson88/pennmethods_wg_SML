{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: intro to supervised machine learning (binary classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn import tree\n",
    "\n",
    "## print mult things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## random\n",
    "import random\n",
    "\n",
    "## import my user-defined functions from the separate .py script\n",
    "from textprocess_utils import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load either raw or preprocessed data\n",
    "\n",
    "\n",
    "**Note**: make sure to change your path if you need to; you can use the following command to check your current path: `os.getcwd()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you want to get more visibility into the preprocessing step from\n",
    "## the raw yelp reviews, set the flag to True \n",
    "PREPROCESS_RAW = False\n",
    "if PREPROCESS_RAW:\n",
    "    yelp = pd.read_pickle(\"../data/yelp_forML.pkl\")\n",
    "    list_stopwords = stopwords.words(\"english\")\n",
    "    yelp['process_text'] = [processtext(one_review, stop_list = list_stopwords) \n",
    "                            for one_review in yelp['raw_text']]\n",
    "    yelp_dtm_init = create_dtm(yelp['process_text'], yelp[['metadata_label', 'metadata_rowid',\n",
    "                                                 'process_text', 'raw_text']])\n",
    "    yelp_dtm = yelp_dtm_init[[col for col in yelp_dtm_init.columns if \n",
    "                             col != \"index\"]] \n",
    "    yelp_dtm.to_csv(\"../data/yelp_forML_preprocessed.csv\",\n",
    "                   index = False)\n",
    "else:\n",
    "    yelp_dtm = pd.read_csv(\"../data/yelp_forML_preprocessed.csv\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Split into features, labels, and split into training/hold out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Split into X (features) and y (labels)\n",
    "\n",
    "- `metadata_label`: the label\n",
    "- `metadata_rowid`: the unique identifier for each observation (a review)\n",
    "- `raw_text`: raw text of review\n",
    "- `process_text`: text after preprocessing\n",
    "- All other columns: terms \n",
    "\n",
    "We keep the `metadata_rowid` for now to help w/ merges even though we won't use it in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in yelp_dtm.columns if col not in ['metadata_label']]\n",
    "X = yelp_dtm[feature_cols].copy()\n",
    "y = yelp_dtm[['metadata_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking dimensionality (note that there would be many more than ~200ish words\n",
    "## if we didn't requre terms appear in >= 5% of documents in the preprocessing)\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert y.shape[1] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create train-test split\n",
    "\n",
    "Two options:\n",
    "- Using the built-in function within sklearn - `train_test_split`\n",
    "- Using a more manual approach that has greater flexibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using built-in function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### more manually: useful when we want more control\n",
    "### over the ids (eg clustering or time ordering)\n",
    "### or if we want to go back to matrix before preprocessing\n",
    "nrows_train = round(X.shape[0]*0.8)\n",
    "nrows_test = X.shape[0] - nrows_train\n",
    "random.seed(221)\n",
    "train_ids = random.sample(set(X['metadata_rowid']), nrows_train)\n",
    "\n",
    "## function that splits given a list of training set ids \n",
    "def my_split(train_ids, id_col):\n",
    "    \n",
    "    ## get test ids\n",
    "    test_ids = set(X[id_col]).difference(train_ids)\n",
    "    \n",
    "    ## split\n",
    "    X_train_man = X[X[id_col].isin(train_ids)].copy()\n",
    "    X_test_man = X[X[id_col].isin(test_ids)].copy()\n",
    "    y_train_man = y[y.index.isin(train_ids)].iloc[:, 0].to_numpy()\n",
    "    y_test_man = y[y.index.isin(test_ids)].iloc[:, 0].to_numpy()\n",
    "    \n",
    "    ## return\n",
    "    return(X_train_man, X_test_man, y_train_man, y_test_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_man, X_test_man, y_train_man, y_test_man = my_split(train_ids, \n",
    "                                                           'metadata_rowid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Estimate model: one model and hardcoding the hyperparameters (decision tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Estimate model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create list of columns present in the training matrix\n",
    "## that are not actually features\n",
    "non_feat = ['metadata_rowid', 'raw_text', 'process_text']\n",
    "\n",
    "## initialize the classifier - this is step where we feed it hyperparameters\n",
    "## use random_state for reproducibility given stochastic element\n",
    "## see here for more parameters that can be varied: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "dt = DecisionTreeClassifier(random_state=0, max_depth = 10)\n",
    "\n",
    "## use the initialized classifier to fit the model\n",
    "X_features = X_train_man[[col for col in X_train_man.columns if col not in \n",
    "                   non_feat]]\n",
    "dt.fit(X_features, y_train_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Generate predictions in validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(X_test_man[[col for col \n",
    "                in X_test_man.columns if col not in non_feat]])\n",
    "y_predprob = dt.predict_proba(X_test_man[[col for col \n",
    "                in X_test_man.columns if col not in non_feat]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the results  (note: changed preprocessing a bit so values differ\n",
    "## from those on slides)\n",
    "y_pred[0:10]\n",
    "y_predprob[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clean up predictions and calculate error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make into a dataframe\n",
    "y_pred_df = pd.DataFrame({'y_pred_binary': y_pred,\n",
    "                         'y_pred_continuous': [one_prob[1] \n",
    "                                            for one_prob in y_predprob],\n",
    "                         'y_true': y_test_man})\n",
    "y_pred_df.sample(n = 10, random_state = 4484)\n",
    "\n",
    "## plot prob versus true\n",
    "(ggplot(y_pred_df, aes(x = 'y_pred_continuous', group = 'factor(y_true)',\n",
    "                      fill = 'factor(y_true)')) +\n",
    "geom_histogram(alpha = 0.2, position = \"dodge\", color = 'black'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision as tp / tp+fp \n",
    "error_cond = [(y_pred_df['y_true'] == 1) & (y_pred_df['y_pred_binary'] == 1),\n",
    "             (y_pred_df['y_true'] == 1) & (y_pred_df['y_pred_binary'] == 0),\n",
    "              (y_pred_df['y_true'] == 0) & (y_pred_df['y_pred_binary'] == 0)]\n",
    "\n",
    "error_codeto = [\"TP\", \"FN\", \"TN\"]\n",
    "\n",
    "y_pred_df['error_cat'] = np.select(error_cond, error_codeto, default = \"FP\")\n",
    "y_error = y_pred_df.error_cat.value_counts().reset_index()\n",
    "y_error\n",
    "y_error.columns = ['cat', 'n']\n",
    "\n",
    "### precision\n",
    "print(\"Precision is:-----------\")\n",
    "y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0]/(y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0] +\n",
    "                    y_error.loc[y_error.cat == \"FP\", 'n'].iloc[0])\n",
    "\n",
    "### recall\n",
    "print(\"Recall is:---------------\")\n",
    "y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0]/(y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0] +\n",
    "                    y_error.loc[y_error.cat == \"FN\", 'n'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interpret the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get top words--- note that feature importance is UNSIGNED\n",
    "## meaning they can predict either pos or neg label\n",
    "feat_imp = pd.DataFrame({'feature_imp': dt.feature_importances_,\n",
    "                         'feature_name': \n",
    "                        [col for col in X_train.columns if col not in non_feat]})\n",
    "feat_imp.sort_values(by = 'feature_imp', ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 variation across positive/negatively-labeled reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at mean counts of top words grouped by whether the \n",
    "## review has a positive label or not \n",
    "top_feat = feat_imp.sort_values(by = 'feature_imp', ascending = False)[0:10]\n",
    "top_feat_list = top_feat.feature_name.to_list()\n",
    "all_agg = [yelp_dtm.groupby(['metadata_label']).agg({one_feat: np.mean})\n",
    "for one_feat in top_feat_list]\n",
    "all_agg_df = pd.concat(all_agg, axis = 1)\n",
    "all_agg_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Tree structure\n",
    "\n",
    "Even though the actual tree is deeper, we're just visualizing the first few splits using the `max_depth` parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt, feature_names= X_features.columns,\n",
    "                   class_names = True,\n",
    "                   filled=True,\n",
    "                  max_depth = 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Estimate model: one model and hardcoding the hyperparameters (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lasso = LogisticRegression(penalty = \"l1\",max_iter=100, \n",
    "             C = 0.01, solver='liblinear')\n",
    "logit_lasso.fit(X_features, y_train_man)\n",
    "\n",
    "y_pred_las = logit_lasso.predict(X_test_man[[col for col in X_test_man.columns \n",
    "                if col not in non_feat]])\n",
    "y_predprob_las = logit_lasso.predict_proba(X_test_man[[col for col in X_test_man.columns\n",
    "                if col not in non_feat]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot predictions- can see smoother than dt \n",
    "y_pred_df_las = pd.DataFrame({'y_pred_binary': y_pred_las,\n",
    "                         'y_pred_continuous': [prob[1] for prob in y_predprob_las],\n",
    "                         'y_true': y_test_man,\n",
    "                        'model': 'lasso'})\n",
    "\n",
    "(ggplot(y_pred_df_las, aes(x = 'y_pred_continuous', group = 'factor(y_true)',\n",
    "                      fill = 'factor(y_true)')) +\n",
    "geom_histogram(alpha = 0.2, position = \"dodge\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get top features\n",
    "las_coef = pd.DataFrame({'coef': logit_lasso.coef_[0],\n",
    "                         'feature_name': \n",
    "                        [col for col in X_train.columns if col not in non_feat]})\n",
    "las_coef.sort_values(by = 'coef', ascending = False)\n",
    "\n",
    "## merge with top features from decision tree\n",
    "## to get a sense of sign of highly-important dt features\n",
    "top_both = pd.merge(las_coef, feat_imp, on = 'feature_name',\n",
    "                   suffixes = [\"_lasso\", \"_dt\"])\n",
    "top_both.sort_values(by = 'feature_imp', ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparing performance across diff hyperparameters: LASSO\n",
    "\n",
    "Rather than hardcoding a single parameter, we're iterating over a range of penalty terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list = np.linspace(4, 0.0001, 10)\n",
    "\n",
    "## define function that takes in one cost parameter\n",
    "## and estimates model, returning pred\n",
    "def one_las(one_c):\n",
    "    one_lasso = LogisticRegression(penalty = \"l1\", max_iter=100, \n",
    "             C = one_c, solver='liblinear')\n",
    "    one_lasso.fit(X_features, y_train_man)\n",
    "    y_pred = one_lasso.predict(X_test_man[[col for col \n",
    "                in X_test_man.columns \n",
    "                if col not in non_feat]])\n",
    "    y_pred_df = pd.DataFrame({'y_pred': y_pred, \n",
    "                             'y_true': y_test_man,\n",
    "                             'cost': one_c})\n",
    "    return(y_pred_df)\n",
    "\n",
    "ESTIMATE_MODEL = False\n",
    "if ESTIMATE_MODEL == True:\n",
    "    all_pred = [one_las(one_c) for one_c in c_list]\n",
    "    with open('../data/lasso_iterate.pkl', 'wb') as handle:\n",
    "        pickle.dump(all_pred, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('../data/lasso_iterate.pkl', 'rb') as handle:\n",
    "        all_pred = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bind into one dataframe \n",
    "all_pred_df = pd.concat(all_pred)\n",
    "all_pred_df.head()\n",
    "## score one cost level \n",
    "def score_onedf(one_c, all_c):\n",
    "    one_df = all_c[all_c.cost == one_c].copy()\n",
    "    prec_onec =  precision_score(one_df['y_true'], one_df['y_pred'])\n",
    "    return(prec_onec)\n",
    "    \n",
    "all_score = pd.DataFrame({'cost': c_list,\n",
    "                         'precision': [score_onedf(one_c, all_pred_df) \n",
    "                                  for one_c in c_list]})\n",
    "all_score\n",
    "\n",
    "all_score[all_score.precision == np.max(all_score.precision)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Activity \n",
    "\n",
    "- Read the documentation here to initialize a ridge regression ($L_{2}$ penalty)- you can use the same cost parameter (C) and number of iterations as in the lasso example above: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Fit the model on the same training data and label as above\n",
    "- Generate binary and continuous predictions\n",
    "- Create a function that takes in a dataframe of binary predictions and true labels and manually calculates the $F_{1}$ score:\n",
    "\n",
    "$$F_{1} = 2 * \\dfrac{precision * recall}{precision + recall} = \\dfrac{TP}{TP + 0.5(FP + FN)}$$\n",
    "\n",
    "- Apply that function to calculate the F1 score for the decision tree and lasso (from above), and ridge regression (from the activity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
